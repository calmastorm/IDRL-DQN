{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee552504",
   "metadata": {},
   "source": [
    "### keynotes\n",
    "\n",
    "In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output. \n",
    "\n",
    "<img src=\"./figures/03.png\" alt=\"DataSet\" title=\"DataSet\" width=\"600\" height=\"300\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7d8de",
   "metadata": {},
   "source": [
    "### packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0893ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "#########################################################################################################\n",
    "########################################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "import gym\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b71357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa86daf",
   "metadata": {},
   "source": [
    "### Q-network\n",
    "\n",
    "\n",
    "<img src=\"./figures/04.png\" alt=\"DataSet\" title=\"DataSet\" width=\"600\" height=\"300\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1635f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "#########################################################################################################\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self,state_num,action_num):\n",
    "        \n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # The first fully connected layer\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_num, 128)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # The second fully connected layer\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        self.out = nn.Linear(128,action_num)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        \n",
    "        action_prob = self.out(x)\n",
    "        \n",
    "        return action_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0319a4",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Experience Replay is the act of storing and replaying game states (the state, action, reward, next_state) that the RL algorithm is able to learn from. \n",
    "\n",
    "Experience Replay can be used in Off-Policy algorithms to learn in an offline fashion. \n",
    "\n",
    "\n",
    "Off-policy methods are able to update the algorithm’s parameters using saved and stored information from previously taken actions. \n",
    "\n",
    "\n",
    "Deep Q-Learning uses Experience Replay to learn in small batches in order to avoid skewing the dataset distribution of different states, actions, rewards, and next_states that the neural network will see. \n",
    "\n",
    "Importantly, the agent doesn’t need to train after each step. \n",
    "\n",
    "In our implementation, we use Experience Replay to train on every single step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "651a0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62273c03",
   "metadata": {},
   "source": [
    "### Deep Q Network algorithm\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./figures/02.png\" alt=\"DataSet\" title=\"DataSet\" width=\"600\" height=\"300\" />\n",
    "\n",
    "#### Update the Q-function using the Bellman Equation\n",
    "\n",
    "The Bellman Equation tells us how to update our Q-table after each step we take. \n",
    "\n",
    "To summarize this equation, the agent updates the current perceived value with the estimated optimal future reward which assumes that the agent takes the best current known action. \n",
    "\n",
    "In an implementation, the agent will search through all the actions for a particular state and choose the state-action pair with the highest corresponding Q-value.\n",
    "\n",
    "\\begin{equation}\n",
    "y_{k} = r_{k} + \\gamma \\cdot Q^{'}(s_{k},\\pi^{'}(s_{k},a_{k}|\\delta^{Q^{'}})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Then, the network is trained by the loss function that is described as\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\delta^{Q})=\\frac{1}{k}\\cdot\\sum_{K}(y_{k}-Q(s_{k},a_{k}|\\delta^{Q}))^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$Q$ and $\\delta^{Q}$: network for prediction\n",
    "\n",
    "$Q^{'}$ and $\\delta^{Q^{'}}$: target network\n",
    "\n",
    "\n",
    "#### Choose an action using the Epsilon-Greedy Exploration Strategy\n",
    "\n",
    "A common strategy for tackling the exploration-exploitation tradeoff is the Epsilon Greedy Exploration Strategy.\n",
    "\n",
    "- At every time step when it’s time to choose an action, roll a dice\n",
    "- If the dice has a probability less than epsilon, choose a random action\n",
    "- Otherwise take the best known action at the agent’s current state\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92914c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "class Deep_Q_Network():\n",
    "    \n",
    "    def __init__(self,\\\n",
    "                 env,\\\n",
    "                 gamma=.99,\\\n",
    "                 capacity=10000,\n",
    "                 batch_size=128,\n",
    "                 lr=1e-4):\n",
    "        \n",
    "        super(Deep_Q_Network, self).__init__()\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 00 environment parameters\n",
    "        \n",
    "        self.env=env\n",
    "        \n",
    "        state_num = env.observation_space.shape[0]\n",
    "        \n",
    "        action_num = env.action_space.n\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 01 construct the evaluate and target network\n",
    "        \n",
    "        self.eval_net, self.target_net = Net(state_num,action_num).to(device), Net(state_num,action_num).to(device)\n",
    "        \n",
    "        self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(),lr=lr)\n",
    "        \n",
    "        self.loss_func = nn.SmoothL1Loss()\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 02 experience replay buffer\n",
    "        \n",
    "        self.capacity=capacity\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 03 epsilon greedy strategy\n",
    "        \n",
    "        self.epsilon=.9\n",
    "        \n",
    "        self.epsilon_start = 0.9\n",
    "        \n",
    "        self.epsilon_end = 0.05\n",
    "        \n",
    "        self.epsilon_decay = 1000\n",
    "        \n",
    "        self.steps_done=0\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 04 parameters\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        \n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 05 network update\n",
    "        \n",
    "        self.tau = 0.005\n",
    "        \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 01 update the epsilon\n",
    "        \n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end)*math.exp(-1. * self.steps_done/self.epsilon_decay)\n",
    "        \n",
    "        self.steps_done+=1\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "\n",
    "        # 02 choose an action: epsilon greedy policy\n",
    "        \n",
    "        if random.random()>=self.epsilon:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "            \n",
    "                action = self.eval_net.forward(state).max(1)[1].view(1, 1)\n",
    "            \n",
    "        else: \n",
    "            \n",
    "            action = torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        if len(self.memory) < self.batch_size:\n",
    "            \n",
    "            return\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 01 randomly sample batch from memory\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        \n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        \n",
    "        action_batch = torch.cat(batch.action)\n",
    "        \n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "\n",
    "        # 02 compute loss \n",
    "        \n",
    "        q_eval = self.eval_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        q_next = torch.zeros(self.batch_size, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            q_next[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * q_next\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        loss = self.loss_func(q_eval, q_target.unsqueeze(1))\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_value_(self.eval_net.parameters(), 100)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #########################################################################################################\n",
    "        #########################################################################################################\n",
    "        \n",
    "        # 03 update the parameters of target Q network\n",
    "            \n",
    "        eval_net_state_dict =  self.eval_net.state_dict()\n",
    "        \n",
    "        target_net_state_dict =  self.target_net.state_dict()\n",
    "            \n",
    "        for key in eval_net_state_dict:\n",
    "            \n",
    "            target_net_state_dict[key] = eval_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "            \n",
    "        self.target_net.load_state_dict(target_net_state_dict)    \n",
    "\n",
    "        \n",
    "    def save(self):\n",
    "        \n",
    "        torch.save(self.eval_net.state_dict(), 'Deep_Q_Network.pth')\n",
    "        \n",
    "        print(\"====================================\")\n",
    "        \n",
    "        print(\"DQN model has been saved...\")\n",
    "        \n",
    "        print(\"====================================\")\n",
    "        \n",
    "\n",
    "    def load(self):\n",
    "        \n",
    "        self.eval_net.load_state_dict(torch.load('Deep_Q_Network.pth'))\n",
    "        \n",
    "        print(\"====================================\")\n",
    "        \n",
    "        print(\"DQN model has been loaded...\")\n",
    "        \n",
    "        print(\"====================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a60527",
   "metadata": {},
   "source": [
    "### hyper-parameters\n",
    "\n",
    "### environment\n",
    "\n",
    "#### game\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "#### action Space\n",
    "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
    "\n",
    "#### Rewards\n",
    "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "#### Starting State\n",
    "All observations are assigned a uniformly random value in (-0.05, 0.05)\n",
    "\n",
    "#### Episode End\n",
    "\n",
    "The episode ends if any one of the following occurs:\n",
    "\n",
    "- Termination: Pole Angle is greater than ±12°\n",
    "\n",
    "- Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "\n",
    "- Truncation: Episode length is greater than 500 (200 for v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "484bb746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da1bf7",
   "metadata": {},
   "source": [
    "### simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7afa89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "DQN model has been loaded...\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "agent=Deep_Q_Network(env)\n",
    "\n",
    "agent.load()\n",
    "\n",
    "agent.steps_done=10000000000000\n",
    "\n",
    "########################################################################################\n",
    "########################################################################################\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in count():\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    action = agent.choose_action(state)\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    if done:\n",
    "\n",
    "        next_state = None\n",
    "\n",
    "    else:\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(1 / 30)\n",
    "\n",
    "    ########################################################################################\n",
    "\n",
    "    if done:\n",
    "        \n",
    "        break\n",
    "            \n",
    "\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c80a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031d9f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
